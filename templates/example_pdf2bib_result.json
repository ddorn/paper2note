{
    "identifier": "10.48550/arXiv.2303.04137",
    "identifier_type": "arxiv DOI",
    "path": "C:\\Users\\mohes\\AppData\\Local\\Temp\\tmpv06uib9b\\2303.04137.pdf",
    "method": "filename + arxiv2doi",
    "validation_info": {
        "id": "http://arxiv.org/abs/2303.04137v5",
        "guidislink": true,
        "link": "http://arxiv.org/abs/2303.04137v5",
        "updated": "2024-03-14T04:36:31Z",
        "updated_parsed": [
            2024,
            3,
            14,
            4,
            36,
            31,
            3,
            74,
            0
        ],
        "published": "2023-03-07T18:50:03Z",
        "published_parsed": [
            2023,
            3,
            7,
            18,
            50,
            3,
            1,
            66,
            0
        ],
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "title_detail": {
            "type": "text/plain",
            "language": null,
            "base": "http://export.arxiv.org/api/query?search_query=id:2303.04137",
            "value": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"
        },
        "summary": "This paper introduces Diffusion Policy, a new way of generating robot\nbehavior by representing a robot's visuomotor policy as a conditional denoising\ndiffusion process. We benchmark Diffusion Policy across 12 different tasks from\n4 different robot manipulation benchmarks and find that it consistently\noutperforms existing state-of-the-art robot learning methods with an average\nimprovement of 46.9%. Diffusion Policy learns the gradient of the\naction-distribution score function and iteratively optimizes with respect to\nthis gradient field during inference via a series of stochastic Langevin\ndynamics steps. We find that the diffusion formulation yields powerful\nadvantages when used for robot policies, including gracefully handling\nmultimodal action distributions, being suitable for high-dimensional action\nspaces, and exhibiting impressive training stability. To fully unlock the\npotential of diffusion models for visuomotor policy learning on physical\nrobots, this paper presents a set of key technical contributions including the\nincorporation of receding horizon control, visual conditioning, and the\ntime-series diffusion transformer. We hope this work will help motivate a new\ngeneration of policy learning techniques that are able to leverage the powerful\ngenerative modeling capabilities of diffusion models. Code, data, and training\ndetails is publicly available diffusion-policy.cs.columbia.edu",
        "summary_detail": {
            "type": "text/plain",
            "language": null,
            "base": "http://export.arxiv.org/api/query?search_query=id:2303.04137",
            "value": "This paper introduces Diffusion Policy, a new way of generating robot\nbehavior by representing a robot's visuomotor policy as a conditional denoising\ndiffusion process. We benchmark Diffusion Policy across 12 different tasks from\n4 different robot manipulation benchmarks and find that it consistently\noutperforms existing state-of-the-art robot learning methods with an average\nimprovement of 46.9%. Diffusion Policy learns the gradient of the\naction-distribution score function and iteratively optimizes with respect to\nthis gradient field during inference via a series of stochastic Langevin\ndynamics steps. We find that the diffusion formulation yields powerful\nadvantages when used for robot policies, including gracefully handling\nmultimodal action distributions, being suitable for high-dimensional action\nspaces, and exhibiting impressive training stability. To fully unlock the\npotential of diffusion models for visuomotor policy learning on physical\nrobots, this paper presents a set of key technical contributions including the\nincorporation of receding horizon control, visual conditioning, and the\ntime-series diffusion transformer. We hope this work will help motivate a new\ngeneration of policy learning techniques that are able to leverage the powerful\ngenerative modeling capabilities of diffusion models. Code, data, and training\ndetails is publicly available diffusion-policy.cs.columbia.edu"
        },
        "authors": [
            {
                "name": "Cheng Chi"
            },
            {
                "name": "Zhenjia Xu"
            },
            {
                "name": "Siyuan Feng"
            },
            {
                "name": "Eric Cousineau"
            },
            {
                "name": "Yilun Du"
            },
            {
                "name": "Benjamin Burchfiel"
            },
            {
                "name": "Russ Tedrake"
            },
            {
                "name": "Shuran Song"
            }
        ],
        "author_detail": {
            "name": "Shuran Song"
        },
        "author": "Shuran Song",
        "arxiv_comment": "An extended journal version of the original RSS2023 paper",
        "links": [
            {
                "href": "http://arxiv.org/abs/2303.04137v5",
                "rel": "alternate",
                "type": "text/html"
            },
            {
                "title": "pdf",
                "href": "http://arxiv.org/pdf/2303.04137v5",
                "rel": "related",
                "type": "application/pdf"
            }
        ],
        "arxiv_primary_category": {
            "term": "cs.RO",
            "scheme": "http://arxiv.org/schemas/atom"
        },
        "tags": [
            {
                "term": "cs.RO",
                "scheme": "http://arxiv.org/schemas/atom",
                "label": null
            }
        ]
    },
    "metadata": {
        "title": "Diffusion Policy: Visuomotor Policy Learning via Action Diffusion",
        "published": "2023-03-07T18:50:03Z",
        "ejournal": "arXiv",
        "ENTRYTYPE": "article",
        "url": "http://arxiv.org/abs/2303.04137v5",
        "doi": null,
        "year": "2023",
        "month": "03",
        "day": "07",
        "author": [
            {
                "given": "Cheng",
                "family": "Chi"
            },
            {
                "given": "Zhenjia",
                "family": "Xu"
            },
            {
                "given": "Siyuan",
                "family": "Feng"
            },
            {
                "given": "Eric",
                "family": "Cousineau"
            },
            {
                "given": "Yilun",
                "family": "Du"
            },
            {
                "given": "Benjamin",
                "family": "Burchfiel"
            },
            {
                "given": "Russ",
                "family": "Tedrake"
            },
            {
                "given": "Shuran",
                "family": "Song"
            }
        ]
    },
    "bibtex": "@article{chi2023diffusion,\n\ttitle = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},\n\tpublished = {2023-03-07T18:50:03Z},\n\tejournal = {arXiv},\n\turl = {http://arxiv.org/abs/2303.04137v5},\n\tyear = {2023},\n\tmonth = {03},\n\tday = {07},\n\tauthor = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song}\n}"
}